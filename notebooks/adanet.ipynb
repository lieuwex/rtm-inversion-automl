{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115294fd",
   "metadata": {},
   "source": [
    "# messy code ahead\n",
    "\n",
    "This notebook contains the code that was developed for the thesis.\n",
    "\n",
    "We combine AdaNet using a Bayesian optimizer from skopt to form a neural network efficiently.\n",
    "\n",
    "However, since this code was not used in the final thesis the code might lack in quality and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ef5cd",
   "metadata": {
    "id": "925ef5cd"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xPL4dKnMOdKe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xPL4dKnMOdKe",
    "outputId": "d148e95e-90e8-434c-bda1-90d88492e1bc"
   },
   "outputs": [],
   "source": [
    "!pip3 install prosail\n",
    "!pip3 install --upgrade scikit-learn scipy numpy adanet==0.9.0 dask distributed SALib tensorflow parameter-sherpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717dc79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a717dc79",
    "outputId": "cbab2ea3-cdec-4737-a580-44ee775e5ae8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import prosail as prosaillib\n",
    "from scipy import stats\n",
    "from scipy.stats import truncnorm\n",
    "import adanet\n",
    "from sklearn.preprocessing import normalize\n",
    "#import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "#tf.get_logger().setLevel('INFO')\n",
    "import skopt\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd770e",
   "metadata": {
    "id": "dedd770e"
   },
   "outputs": [],
   "source": [
    "def biomass_from_height(height):\n",
    "    #return (14.706*height - 12.094) / 10000\n",
    "    return -0.0120942 + 0.0147056 * height\n",
    "\n",
    "def biomass_from_lai_and_lma(lai, lma):\n",
    "    return lai * lma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ad6c1",
   "metadata": {
    "id": "ce1ad6c1"
   },
   "source": [
    "### Load random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597c25c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a597c25c",
    "outputId": "fbb862d3-5b64-46ee-f1f6-161a79f4081a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./calculated_prosail_combined.csv', index_col=0).head(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7803b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "calculated = df.to_numpy()\n",
    "X = calculated[:, :9]\n",
    "Y = calculated[:, 10:]\n",
    "Y = np.array([ row[0]*row[1] for row in Y ])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, shuffle=False)\n",
    "\n",
    "print(x_train[0], '->', y_train[0])\n",
    "\n",
    "print('training samples', x_train.shape[0], 'test samples', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f7652",
   "metadata": {
    "id": "9e7f7652"
   },
   "source": [
    "### Fit the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = '/tmp/'\n",
    "\n",
    "import shutil\n",
    "\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "\n",
    "print(\"running\")\n",
    "\n",
    "## Install ngrok binary.\n",
    "#! rm -f ngrok-stable-linux-amd64.zip ngrok\n",
    "#! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "#! unzip ngrok-stable-linux-amd64.zip\n",
    "#\n",
    "## Delete old logs dir.\n",
    "#shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
    "#\n",
    "#print(\"Follow this link to open TensorBoard in a new tab.\")\n",
    "#get_ipython().system_raw('./ngrok http 6006 &')\n",
    "#!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adanet.subnetwork import Generator\n",
    "from skopt.space.space import Integer\n",
    "\n",
    "adanet_dimensions = [\n",
    "    Integer(low=1, high=100, name='dnn_layer_one'),\n",
    "    Integer(low=-15, high=100, name='dnn_layer_two'),\n",
    "    Integer(low=-15, high=100, name='dnn_layer_three'),\n",
    "    #Integer(low=-15, high=200, name='dnn_layer_four'),\n",
    "    #Integer(low=-15, high=100, name='dnn_layer_five')\n",
    "]\n",
    "\n",
    "opts = {} # iteration -> gp_object\n",
    "info_points = { 0: [] } # iteration -> [(dnn_params, loss)]\n",
    "\n",
    "class GPGenerator(Generator):\n",
    "    def __init__(self):\n",
    "        super()\n",
    "        \n",
    "    def generate_candidates(self, previous_ensemble, iteration_number, previous_ensemble_reports, all_reports, config):\n",
    "        global next_iter, info_for_next_iter\n",
    "        print('iteration', iteration_number)\n",
    "        \n",
    "        if iteration_number+1 not in info_points:\n",
    "            info_points[iteration_number+1] = []\n",
    "        \n",
    "        opt = opts.get(iteration_number)\n",
    "        if opt is None:\n",
    "            n_initial_points = 9 if iteration_number == 0 else 0\n",
    "            opts[iteration_number] = skopt.optimizer.Optimizer(dimensions=adanet_dimensions, base_estimator=\"GP\", initial_point_generator=\"sobol\", n_initial_points=n_initial_points)\n",
    "            opt = opts[iteration_number]\n",
    "            \n",
    "            #for (params, loss) in info_points[iteration_number]:\n",
    "            for report in previous_ensemble_reports:\n",
    "                if not report.name.startswith(\"dnn_\"):\n",
    "                    continue\n",
    "                loss = report.metrics['loss']\n",
    "                params = [int(x) for x in report.name.split('_')[1:]]\n",
    "                print('telling', params, loss)\n",
    "                opt.tell(params, loss)\n",
    "        \n",
    "        for report in previous_ensemble_reports:\n",
    "            if not report.name.startswith(\"dnn_\"):\n",
    "                continue\n",
    "                \n",
    "            loss = report.metrics['loss']\n",
    "            params = [int(x) for x in report.name.split('_')[1:]]\n",
    "            print('storing', params, loss)\n",
    "            info_points[iteration_number].append((params, loss))\n",
    "        \n",
    "        def conv(name, x):\n",
    "            return adanet.autoensemble.common._BuilderFromSubestimator(\n",
    "                name,\n",
    "                adanet.autoensemble.common._convert_to_subestimator(x),\n",
    "                logits_fn=adanet.autoensemble.common._default_logits,\n",
    "                last_layer_fn=None,\n",
    "                config=config)\n",
    "        \n",
    "        res = [conv(\"linear\", tf.estimator.LinearEstimator(\n",
    "                head=head,\n",
    "                feature_columns=feature_columns,\n",
    "                config=config))]\n",
    "        \n",
    "        points = opt.ask(n_points=9)\n",
    "        used = set()\n",
    "        for point in points:\n",
    "            #name = \"dnn_{}_{}_{}_{}_{}\".format(point[0], point[1], point[2], point[3], point[4])\n",
    "            name = \"dnn_{}_{}_{}\".format(point[0], point[1], point[2])\n",
    "            if name in used:\n",
    "                continue\n",
    "            used.add(name)\n",
    "            \n",
    "            new_point = [point[0]]\n",
    "            for i in range(1, len(point)):\n",
    "                new_point.append(new_point[-1] - point[i])\n",
    "            point = new_point\n",
    "            \n",
    "            print(\"adding\", name, end=\" \")\n",
    "            \n",
    "            hidden_units = []\n",
    "            for x in point:\n",
    "                if x <= 0:\n",
    "                    break\n",
    "                hidden_units.append(x)\n",
    "            print(hidden_units)\n",
    "                \n",
    "            item = tf.estimator.DNNEstimator(\n",
    "                    head=head,\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config,\n",
    "                    hidden_units=hidden_units)\n",
    "            \n",
    "            res.append(conv(name, item))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U14y3m62qcEn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U14y3m62qcEn",
    "outputId": "4c7e007c-5ef5-406d-d390-01dc8b58459a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "FEATURES_KEY = \"x\"\n",
    "\n",
    "#col_names = ['solar_zenith', 'solar_azimuth', 'sensor_zenith', 'sensor_azimuth', 'B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'B8']\n",
    "col_names = [ \"x\" ]\n",
    "feature_columns = [ tf.feature_column.numeric_column(x, shape=[11,1]) for x in col_names ]\n",
    "\n",
    "head = tf.estimator.RegressionHead(label_dimension=1)\n",
    "generator = GPGenerator()\n",
    "\n",
    "def f(config):\n",
    "    print('config', config)\n",
    "    return {\n",
    "        \"linear\":\n",
    "            tf.estimator.LinearEstimator(\n",
    "                head=head,\n",
    "                feature_columns=feature_columns,\n",
    "                config=config),\n",
    "        \"dnn\":\n",
    "            tf.estimator.DNNEstimator(\n",
    "                head=head,\n",
    "                feature_columns=feature_columns,\n",
    "                config=config,\n",
    "                hidden_units=[25]),\n",
    "        \"dnn2_50-25\":\n",
    "            tf.estimator.DNNEstimator(\n",
    "                head=head,\n",
    "                feature_columns=feature_columns,\n",
    "                config=config,\n",
    "                hidden_units=[50, 25]),\n",
    "        \"dnn2_100-25\":\n",
    "            tf.estimator.DNNEstimator(\n",
    "                head=head,\n",
    "                feature_columns=feature_columns,\n",
    "                config=config,\n",
    "                hidden_units=[100, 25]),\n",
    "        \"dnn3\":\n",
    "            tf.estimator.DNNEstimator(\n",
    "                head=head,\n",
    "                feature_columns=feature_columns,\n",
    "                config=config,\n",
    "                hidden_units=[200, 100, 25]),\n",
    "    }\n",
    "\n",
    "def input_fn_train():\n",
    "    dataset = tf.compat.v1.data.Dataset.from_tensor_slices(({\n",
    "        FEATURES_KEY: x_train\n",
    "    }, y_train))\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset = dataset.batch(32)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "def input_fn_eval():\n",
    "    dataset = tf.compat.v1.data.Dataset.from_tensor_slices(({\n",
    "        FEATURES_KEY: x_test\n",
    "    }, y_test))\n",
    "    dataset = dataset.batch(32)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "estimator = adanet.Estimator(\n",
    "    head=head,\n",
    "    #candidate_pool=[],\n",
    "    subnetwork_generator=generator,\n",
    "    max_iteration_steps=60,\n",
    "    report_materializer=adanet.ReportMaterializer(input_fn_eval),\n",
    "    max_iterations=15\n",
    ")\n",
    "\n",
    "estimator.train(input_fn=input_fn_train)\n",
    "metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012be076",
   "metadata": {
    "id": "012be076"
   },
   "source": [
    "#### ... and evaluate the result on other artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d612a26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "id": "4d612a26",
    "outputId": "45be6136-b360-4da4-a058-16d7d31d21ec"
   },
   "outputs": [],
   "source": [
    "def input_fn_eval():\n",
    "    dataset = tf.compat.v1.data.Dataset.from_tensor_slices(({\n",
    "        FEATURES_KEY: x_test\n",
    "    }, y_test))\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset = dataset.batch(32)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "#predicted = list(estimator.predict(input_fn=input_fn_eval))\n",
    "#predicted = np.array([ x['predictions'] for x in estimator.predict(input_fn=input_fn_eval) ])\n",
    "predicted = np.array([ x['predictions'] for x in estimator.predict(input_fn=input_fn_eval) ])\n",
    "print(predicted)\n",
    "print(\"score over test set using double var\", sklearn.metrics.mean_absolute_percentage_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933136db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "933136db",
    "outputId": "8458c672-7b85-4861-d868-b167bbbb8a8e"
   },
   "outputs": [],
   "source": [
    "yhat = predicted\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce1877",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "29ce1877",
    "outputId": "dfc068bc-5c32-4cf5-b72a-34185bc97007"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.scatter(y_test, yhat)\n",
    "pyplot.show()\n",
    "\n",
    "\"\"\"\n",
    "pyplot.scatter(y_test[:, 0], yhat[:, 0])\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.scatter(y_test[:, 1], yhat[:, 1])\n",
    "pyplot.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2347c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0878d553",
   "metadata": {
    "id": "0878d553"
   },
   "source": [
    "## Let's now test it on a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe842e97",
   "metadata": {
    "id": "fe842e97"
   },
   "source": [
    "### Load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb505def",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fb505def",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv('/content/drive/MyDrive/colabfiles/data_with_angles.csv')\n",
    "etm_rows = items[items['SensorType'] == 'ETM+']\n",
    "etm_rows = etm_rows.dropna(subset=['h_grass', 'c_grass'])\n",
    "\n",
    "bands = etm_rows.filter(regex='B\\d$') / 10_000\n",
    "grass_info = etm_rows.filter(regex='_grass$')\n",
    "angles = etm_rows.filter(regex='B\\d_.+$')\n",
    "\n",
    "items = grass_info.join(bands).join(angles)\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd9c33",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a6dd9c33",
    "outputId": "08df926f-53c6-4669-809b-4a3570624988"
   },
   "outputs": [],
   "source": [
    "def mean(df):\n",
    "    x = df.to_numpy()\n",
    "    return np.nanmean(x, axis=1)\n",
    "\n",
    "def get_angles(df):\n",
    "    sensor_azimuth = mean(df.filter(regex='B\\d_sensor_azimuth'))\n",
    "    sensor_zenith = mean(df.filter(regex='B\\d_sensor_zenith'))\n",
    "    solar_azimuth = mean(df.filter(regex='B\\d_solar_azimuth'))\n",
    "    solar_zenith = mean(df.filter(regex='B\\d_solar_zenith'))\n",
    "    return np.column_stack((solar_zenith, solar_azimuth, sensor_zenith, sensor_azimuth))\n",
    "\n",
    "x_test_bands = items.filter(regex='B\\d$').to_numpy()\n",
    "x_test_bands = normalize(x_test_bands, norm='max', axis=0)\n",
    "x_test_angles = get_angles(items)\n",
    "x_test_angles = normalize(x_test_angles, norm='max')\n",
    "x_test = np.column_stack((x_test_angles, x_test_bands))\n",
    "\n",
    "y_test = np.array([ biomass_from_height(x[0]) for x in items[['h_grass']].to_numpy() ])\n",
    "#y_test = [ x[0] for x in items[['h_grass']].to_numpy() ]\n",
    "#y_test = [ y if y >= 0 else 0 for y in y_test ]\n",
    "\n",
    "#y_test = items[['h_grass', 'c_grass']].to_numpy()\n",
    "print(x_test[0], '->', y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oBX3jfTt4WgR",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oBX3jfTt4WgR",
    "outputId": "3d01281e-9d65-4505-cd84-1051f17895c9"
   },
   "outputs": [],
   "source": [
    "params.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lu_oykAhOvfS",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lu_oykAhOvfS",
    "outputId": "f1c1365a-f89e-48e9-bc78-689f9cba3e43"
   },
   "outputs": [],
   "source": [
    "x_test_prosail = np.array([ prosail_to_etm(etm_rsr, prosail(row)) for (_,row) in test_params.iterrows() ])\n",
    "x_test_prosail = normalize(x_test_prosail, norm='max', axis=0)\n",
    "pd.DataFrame(x_test_prosail).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xHaf037157vK",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xHaf037157vK",
    "outputId": "fc2b3257-1223-4804-c4ac-c6b5c342fb75"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(x_test_bands).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa13e40",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4fa13e40",
    "outputId": "699da8c5-80ff-474f-d9a9-970f2e986210"
   },
   "outputs": [],
   "source": [
    "predicted = np.array([ biomass_from_lai_and_lma(row[0], row[1]) for row in cls.predict(x_test)])\n",
    "print(\"score over test set using single var\", sklearn.metrics.mean_absolute_percentage_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6eb3c2",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ad6eb3c2",
    "outputId": "e26af85b-a18c-4520-9edd-3c394b0412e5"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "yhat = predicted\n",
    "\n",
    "pyplot.scatter(y_test, yhat)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50297f",
   "metadata": {
    "id": "bd50297f"
   },
   "outputs": [],
   "source": [
    "sgpitats.describe(np.array(x_test)[:1000] - np.array(x_train)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e03f9b",
   "metadata": {
    "id": "f4e03f9b"
   },
   "outputs": [],
   "source": [
    "stats.describe(np.array(y_test)[:1000] - np.array(y_train)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee3f84",
   "metadata": {
    "id": "71ee3f84"
   },
   "outputs": [],
   "source": [
    "res = gpr.predict(x_test)\n",
    "heights = [ 68.0013*m + 0.822421 for m in res ]\n",
    "heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f85853",
   "metadata": {
    "id": "60f85853"
   },
   "outputs": [],
   "source": [
    "np.abs(np.array(y_test) - np.array(heights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fff4e",
   "metadata": {
    "id": "254fff4e"
   },
   "outputs": [],
   "source": [
    "diff = np.abs(items['h_grass'].to_numpy() - np.array(heights))\n",
    "print(stats.describe(diff))\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccee201",
   "metadata": {
    "id": "fccee201"
   },
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6682947b",
   "metadata": {
    "id": "6682947b"
   },
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974e186",
   "metadata": {
    "id": "c974e186"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876a2fd",
   "metadata": {
    "id": "c876a2fd"
   },
   "outputs": [],
   "source": [
    "cls.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f760e",
   "metadata": {
    "id": "f19f760e"
   },
   "outputs": [],
   "source": [
    "test_params[:, -5:-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32acdfc9",
   "metadata": {
    "id": "32acdfc9"
   },
   "outputs": [],
   "source": [
    "import SALib.sample.latin\n",
    "\n",
    "SALib.sample.latin.sample({ 'num_vars': 1, 'names': ['x1'], 'bounds': [[-10, 10]] }, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HjlZ1Jk2mqgm",
   "metadata": {
    "id": "HjlZ1Jk2mqgm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adanet",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
